{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff73961d-baa7-4e85-929f-7641aead0de7",
   "metadata": {},
   "source": [
    "### Machine Learning: Programming Exercise - 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f496dd-efe3-4321-b434-1f8b4a4dab2b",
   "metadata": {},
   "source": [
    "# Exercise 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8259c056-3ab1-455a-8d9a-7b68ed0f93a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Write a function to generate an m+1 dimensional data set, of size n, consisting of m continuous independent\n",
    "# variables (X) and one dependent binary variable (Y) defined as\n",
    "# Y =\n",
    "# (\n",
    "# 1 if p(y = 1|~x) = 1\n",
    "# 1+exp−~x.β~ > 0.5\n",
    "# 0 otherwise\n",
    "# where,\n",
    "# • β is a random vector of dimensionality m + 1, representing the coefficients of the linear relationship\n",
    "# between X and Y, and\n",
    "# • ∀i ∈ [1, n], xi0 = 1\n",
    "# To add noise to the labels (Y) generated, we assume a Bernoulli distribution with probability of success, θ,\n",
    "# that determines whether or not the label generated, as above, is to be flipped. The larger the value of θ, the\n",
    "# greater is the noise.\n",
    "# The function should take the following parameters:\n",
    "# • θ: The probability of flipping the label, Y\n",
    "# • n: The size of the data set\n",
    "# • m: The number of indepedent variables\n",
    "# Output from the function should be:\n",
    "# • X: An n × m numpy array of independent variable values (with a 1 in the first column)\n",
    "# • Y : The n × 1 binary numpy array of output values\n",
    "# • β: The random coefficients used to generate Y from X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a0c69bb-5cfb-4411-8453-95fbc1eb6f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [[ 1.         -0.27329365 -1.05550119  0.15212318]\n",
      " [ 1.         -0.25045361  1.12212036 -1.19715069]\n",
      " [ 1.         -0.39695128  0.03661479 -1.33136218]\n",
      " [ 1.         -0.00808617 -0.04182455  2.82141115]\n",
      " [ 1.         -0.06940533 -0.12249763  0.02720888]\n",
      " [ 1.         -0.39008725  0.59292567 -0.14454713]\n",
      " [ 1.          0.44047834  0.64217533  0.61548929]\n",
      " [ 1.          1.31056003 -0.09592239 -0.38901027]\n",
      " [ 1.         -0.59931308  1.35583867  0.62423856]\n",
      " [ 1.         -0.97523765  1.08398632  0.47226829]\n",
      " [ 1.          0.15357714 -1.23240963  0.53901217]\n",
      " [ 1.          0.59662264  0.44799239  0.16329038]\n",
      " [ 1.          0.32427161 -0.82996629  1.23199848]\n",
      " [ 1.          0.77850915  1.00496193 -0.29395673]\n",
      " [ 1.         -1.18618473 -0.51910343 -0.73749938]\n",
      " [ 1.         -1.90554525  0.90299972  0.3902098 ]\n",
      " [ 1.         -0.5460051  -1.51301071 -0.70422384]\n",
      " [ 1.         -0.28971169 -0.01175759  0.8700295 ]\n",
      " [ 1.         -0.74243247 -0.20810552 -0.89117907]\n",
      " [ 1.          1.04424888  0.37840975 -0.32856335]\n",
      " [ 1.         -1.81510901  1.39205404  1.19119946]\n",
      " [ 1.         -1.5070536   0.04874951  0.49824549]\n",
      " [ 1.          0.16358918 -1.35933139  0.42443831]\n",
      " [ 1.         -0.38934167 -0.77527834 -0.343516  ]\n",
      " [ 1.         -0.25427609  0.93263067 -0.10216102]\n",
      " [ 1.          0.65114498 -0.59948513 -0.42836457]\n",
      " [ 1.         -1.0752762  -0.3907514   0.58669203]\n",
      " [ 1.          0.45249583  0.18503758 -0.39458025]\n",
      " [ 1.          0.45193494 -1.81282665 -3.47345491]\n",
      " [ 1.         -0.42838065 -0.72711288  0.65609921]\n",
      " [ 1.         -0.65570516 -1.28082487 -1.92592738]\n",
      " [ 1.          1.18462759  0.68107673  0.02393382]\n",
      " [ 1.         -0.61433815  0.46686173 -0.82527159]\n",
      " [ 1.          0.81622514 -1.10997901  0.02243681]\n",
      " [ 1.          1.32388928  0.22365529  0.80470084]\n",
      " [ 1.          0.44804913 -0.62754139 -0.21945604]\n",
      " [ 1.         -0.09727932  0.79006838 -0.88514116]\n",
      " [ 1.          0.06514222  1.09261267  1.65075253]\n",
      " [ 1.         -0.52589581 -1.18980293 -0.70475382]\n",
      " [ 1.         -0.17076834  0.56607101 -0.5359554 ]\n",
      " [ 1.         -0.35033799  0.37805381  1.71412286]\n",
      " [ 1.          0.95237978  0.1391504  -1.66184567]\n",
      " [ 1.         -1.07169255 -0.71648298  0.02757236]\n",
      " [ 1.          0.70736022  0.78895636  1.20968453]\n",
      " [ 1.          1.03628123  2.03643102 -1.88230202]\n",
      " [ 1.          0.13109357  0.62474913 -0.00397481]\n",
      " [ 1.         -0.88536186  0.95113322 -0.21202218]\n",
      " [ 1.         -1.10064683  1.67824161 -1.12916786]\n",
      " [ 1.         -1.04467023 -0.00531502 -0.08410775]\n",
      " [ 1.         -2.08208111  1.39245225  2.03525612]\n",
      " [ 1.          0.91703131  0.14303831  1.15589647]\n",
      " [ 1.          0.54914127 -0.45533262 -0.87036724]\n",
      " [ 1.         -0.47529044 -0.25959478 -0.00610124]\n",
      " [ 1.          0.38057306 -0.58499711  0.03193033]\n",
      " [ 1.         -1.57684132  0.10460022 -0.42333221]\n",
      " [ 1.          0.1438897  -0.13447221 -0.12984542]\n",
      " [ 1.          1.58476402 -1.14575447 -0.7967136 ]\n",
      " [ 1.          1.43837836  1.144771   -2.3382391 ]\n",
      " [ 1.         -0.62984512  0.25774369 -0.0748389 ]\n",
      " [ 1.          2.41415355 -2.07317065 -0.23328043]\n",
      " [ 1.         -0.72057552  1.67555284  0.01668473]\n",
      " [ 1.         -1.34517442  0.74897635 -1.73855291]\n",
      " [ 1.         -0.77837864  0.52293109  0.68332064]\n",
      " [ 1.         -0.2032687   0.66074127  0.74259357]\n",
      " [ 1.          1.12121139  1.14471391  1.21931313]\n",
      " [ 1.          1.24238352  0.45763091 -0.8297875 ]\n",
      " [ 1.          0.42747311 -0.03015596 -1.37213665]\n",
      " [ 1.         -1.08561358  1.42280108  1.44256349]\n",
      " [ 1.          0.17141935 -1.15644294 -1.06677991]\n",
      " [ 1.          0.64194192  0.34675368 -0.42896839]\n",
      " [ 1.          1.25991528  0.90167557  0.47035572]\n",
      " [ 1.         -0.1694801   0.529325   -0.40078098]\n",
      " [ 1.          0.22961042  0.47400696 -0.29813817]\n",
      " [ 1.          0.11076557 -0.35851339  0.0226952 ]\n",
      " [ 1.         -1.81455273  1.73769174 -0.82805043]\n",
      " [ 1.         -0.57112679 -0.79946073  1.27068118]\n",
      " [ 1.          0.68203202 -1.06321526 -1.62917205]\n",
      " [ 1.         -0.81798334  0.47586571 -0.4912047 ]\n",
      " [ 1.         -0.03716738 -0.17829764 -0.80875309]\n",
      " [ 1.          0.91964671 -2.09521969 -1.33767222]\n",
      " [ 1.          1.82473243 -1.34393108 -0.98180171]\n",
      " [ 1.         -0.1930694   0.88974629  2.30739456]\n",
      " [ 1.          1.06706931  0.13758976 -0.86193913]\n",
      " [ 1.          1.07482631 -1.50372931 -0.20179018]\n",
      " [ 1.          0.31971516  0.06010991  1.07981186]\n",
      " [ 1.          1.18955986  2.07345033 -1.96915891]\n",
      " [ 1.          0.7857994   0.20792395  0.72159198]\n",
      " [ 1.         -1.40133546 -3.27116679  0.14956495]\n",
      " [ 1.         -0.437991    0.38782322 -0.204325  ]\n",
      " [ 1.         -0.6202311  -1.00365378 -0.8208381 ]\n",
      " [ 1.          0.31249469 -1.20617107  0.64494429]\n",
      " [ 1.         -1.82145466  0.71412428 -0.07866482]\n",
      " [ 1.         -0.25012364 -1.58702318  1.33819336]\n",
      " [ 1.          0.68875778  0.87907141 -0.88839856]\n",
      " [ 1.          0.86488167 -1.49533535 -0.07891939]\n",
      " [ 1.          0.71565672 -0.07093178  1.3744039 ]\n",
      " [ 1.          2.19336754 -0.24652285 -1.08123286]\n",
      " [ 1.          0.47109411  0.29300027  0.84387091]\n",
      " [ 1.         -0.20977095 -0.38773259 -0.39219016]\n",
      " [ 1.         -0.06082885  0.46869305  0.67831565]]\n",
      "Y: [1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 0 1 1 1\n",
      " 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1\n",
      " 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1]\n",
      "Beta: [ 1.49129091 -0.64416542  0.97447439  1.24848363]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_dataset(n, m, theta):\n",
    "    # Step 1: Generate X (n × m array with first column as 1s)\n",
    "    X = np.ones((n, m + 1))\n",
    "    X[:, 1:] = np.random.randn(n, m)  # random values for independent variables\n",
    "    \n",
    "    # Step 2: Generate beta (random coefficients)\n",
    "    beta = np.random.randn(m + 1)\n",
    "    \n",
    "    # Step 3: Calculate probability p(y=1|x)\n",
    "    linear_combination = X.dot(beta)  # X.beta\n",
    "    prob = 1 / (1 + np.exp(-linear_combination))  # Sigmoid function\n",
    "\n",
    "    # Step 4: Generate binary Y using probability threshold of 0.5\n",
    "    Y = np.where(prob > 0.5, 1, 0)\n",
    "    \n",
    "    # Step 5: Add noise (flip labels) using Bernoulli distribution\n",
    "    noise = np.random.binomial(1, theta, n)\n",
    "    Y = np.abs(Y - noise)  # Flip the labels based on noise\n",
    "    \n",
    "    return X, Y, beta\n",
    "\n",
    "X, Y, beta = generate_dataset(n=100, m=3, theta=0.1)\n",
    "print(\"X:\", X)\n",
    "print(\"Y:\", Y)\n",
    "print(\"Beta:\", beta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa216f7-1096-4f93-acab-ecd753736a4e",
   "metadata": {},
   "source": [
    "# Exercise 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb07c0b3-a68b-4e08-8b04-652abd49a6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Write a function that learns the parameters of a logistic regression function given inputs\n",
    "# • X: An n × m numpy array of independent variable values\n",
    "# • Y : The n × 1 binary numpy array of output values\n",
    "# • k: the number of iteractions (epochs)\n",
    "# • τ : the threshold on change in Cost function value from the previous to current iteration\n",
    "# • λ: the learning rate for Gradient Descent\n",
    "# The function should implement the Gradient Descent algorithm as discussed in class that initialises β with\n",
    "# random values and then updates these values in each iteraction by moving in the the direction defined by\n",
    "# the partial derivative of the cost function with respect to each of the coefficients. The function should use\n",
    "# only one loop that ends after a number of iterations (k) or a threshold on the change in cost function value\n",
    "# (τ ).\n",
    "# The output should be a m + 1 dimensional vector of coefficients and the final cost function value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3abd8f1-57f7-44b7-95c7-cb3f57ee2b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned beta: [ 0.70811004 -0.19954538  0.82317049  0.58766335]\n",
      "Final cost: 0.502071261239914\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_cost(X, Y, beta):\n",
    "    n = len(Y)\n",
    "    pred = sigmoid(X.dot(beta))\n",
    "    cost = - (1/n) * np.sum(Y * np.log(pred) + (1 - Y) * np.log(1 - pred))\n",
    "    return cost\n",
    "\n",
    "def logistic_regression(X, Y, k, t, lam): #t->tow lam->lambda\n",
    "    n, m = X.shape\n",
    "    beta = np.random.randn(m)  # Initialize β randomly\n",
    "    prev_cost = float('inf')  # Set to a very large value initially\n",
    "    \n",
    "    for i in range(k):\n",
    "        # Calculate predictions\n",
    "        pred = sigmoid(X.dot(beta))\n",
    "        \n",
    "        # Compute gradient of the cost function\n",
    "        gradient = (1/n) * X.T.dot(pred - Y)   #T refers to the transpose of the matrix X.\n",
    "        \n",
    "        # Update β using gradient descent rule\n",
    "        beta = beta - lam * gradient\n",
    "        \n",
    "        # Calculate current cost\n",
    "        cost = compute_cost(X, Y, beta)\n",
    "        \n",
    "        # Break if change in cost is smaller than the threshold\n",
    "        if abs(prev_cost - cost) < t:\n",
    "            break\n",
    "        \n",
    "        prev_cost = cost\n",
    "    \n",
    "    return beta, cost\n",
    "\n",
    "beta_learned, final_cost = logistic_regression(X, Y, k=1000, t=1e-5, lam=0.01)\n",
    "print(\"Learned beta:\", beta_learned)\n",
    "print(\"Final cost:\", final_cost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0556c4d-1f36-46cb-bdfc-07b1202db52c",
   "metadata": {},
   "source": [
    "# Exercise 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7040d124-bd22-4058-9cca-0ca17b619416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Create a report investigating how different values of n and θ impact the ability for your logistic regression\n",
    "# function to learn the coefficients, β, used to generate the output vector Y . Also include your derivation of\n",
    "# the partial derivative of the cost function with respect to the parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fcf82e9-a93d-4ff5-a11e-0fd21197429c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n=100 and theta=0.1:\n",
      "True beta: [-0.32962422 -0.81516611 -0.83943129  0.99121101]\n",
      "Learned beta: [-0.40591769 -0.39865733 -0.50437026  0.45344487]\n",
      "\n",
      "\n",
      "For n=100 and theta=0.2:\n",
      "True beta: [ 0.26547762  0.80167407 -0.20524437  1.11414271]\n",
      "Learned beta: [-0.06120269  1.16178099  0.30361762  0.93823348]\n",
      "\n",
      "\n",
      "For n=100 and theta=0.5:\n",
      "True beta: [-1.09798409  0.00309739 -1.34560518 -0.37056647]\n",
      "Learned beta: [ 0.21529847  0.33656673 -0.20763275  0.07624289]\n",
      "\n",
      "\n",
      "For n=500 and theta=0.1:\n",
      "True beta: [-1.61791698 -1.70085017 -1.6360979  -0.86240187]\n",
      "Learned beta: [-0.82439696 -0.67168178 -0.81479798 -0.33838537]\n",
      "\n",
      "\n",
      "For n=500 and theta=0.2:\n",
      "True beta: [ 0.55562971  1.06385378  0.65189559 -0.18655418]\n",
      "Learned beta: [ 0.25647295  0.78709422  0.5325284  -0.65575173]\n",
      "\n",
      "\n",
      "For n=500 and theta=0.5:\n",
      "True beta: [ 0.15664084 -0.31705474 -0.19251546 -0.47224244]\n",
      "Learned beta: [-0.18851968  0.35432232 -0.36137937 -0.03835191]\n",
      "\n",
      "\n",
      "For n=1000 and theta=0.1:\n",
      "True beta: [-0.09317351 -0.02808011  0.53010252  0.54168171]\n",
      "Learned beta: [-0.1481847   0.07080099  0.88421474  0.8873547 ]\n",
      "\n",
      "\n",
      "For n=1000 and theta=0.2:\n",
      "True beta: [-0.05336653 -0.31411724 -1.53253112  0.05412242]\n",
      "Learned beta: [-0.16741324  0.02248256 -1.09353568 -0.31745326]\n",
      "\n",
      "\n",
      "For n=1000 and theta=0.5:\n",
      "True beta: [ 0.99711785 -0.48646885  2.51570084  0.20293242]\n",
      "Learned beta: [ 0.11014526 -0.00132754  0.07744875 -0.03760506]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_values = [100, 500, 1000]\n",
    "theta_values = [0.1, 0.2, 0.5]\n",
    "\n",
    "for n in n_values:\n",
    "    for theta in theta_values:\n",
    "        X, Y, beta_true = generate_dataset(n, 3, theta)\n",
    "        beta_learned, _ = logistic_regression(X, Y, k=1000, t=1e-5, lam=0.01)\n",
    "        print(f\"For n={n} and theta={theta}:\")\n",
    "        print(\"True beta:\", beta_true)\n",
    "        print(\"Learned beta:\", beta_learned)\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3ef54c-4bad-42c6-8054-20ddefd34849",
   "metadata": {},
   "source": [
    "# Exercise 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4a54eec-0244-4154-b4b1-df49585ad171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 Add L1 and L2 regularization to the Logistic Regression cost function. How does this impact the models\n",
    "# learnt? How does the choice of regularization constant impact the β vector learned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2587baa3-d9f0-45bb-85f4-94a8456abd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned beta with L2 regularization: [-0.10904501 -0.05453203  0.03698111  0.08082452]\n"
     ]
    }
   ],
   "source": [
    "def compute_cost_regularized(X, Y, beta, alpha, regularization='L2'):\n",
    "    n = len(Y)\n",
    "    pred = sigmoid(X.dot(beta))\n",
    "    cost = - (1/n) * np.sum(Y * np.log(pred) + (1 - Y) * np.log(1 - pred))\n",
    "    \n",
    "    # L1 Regularization\n",
    "    if regularization == 'L1':\n",
    "        cost += alpha * np.sum(np.abs(beta))\n",
    "    \n",
    "    # L2 Regularization\n",
    "    elif regularization == 'L2':\n",
    "        cost += (alpha/2) * np.sum(beta**2)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def logistic_regression_regularized(X, Y, k, t, lam, alpha, regularization='L2'):\n",
    "    n, m = X.shape\n",
    "    beta = np.random.randn(m)\n",
    "    prev_cost = float('inf')\n",
    "    \n",
    "    for i in range(k):\n",
    "        pred = sigmoid(X.dot(beta))\n",
    "        gradient = (1/n) * X.T.dot(pred - Y)\n",
    "        \n",
    "        # Regularization terms added to gradient update\n",
    "        if regularization == 'L1':\n",
    "            gradient += alpha * np.sign(beta)  # Add L1 regularization gradient\n",
    "        elif regularization == 'L2':\n",
    "            gradient += alpha * beta  # Add L2 regularization gradient\n",
    "        \n",
    "        beta = beta - lam * gradient\n",
    "        \n",
    "        cost = compute_cost_regularized(X, Y, beta, alpha, regularization)\n",
    "        \n",
    "        if abs(prev_cost - cost) < t:\n",
    "            break\n",
    "        \n",
    "        prev_cost = cost\n",
    "    \n",
    "    return beta, cost\n",
    "\n",
    "beta_learned_L2, final_cost_L2 = logistic_regression_regularized(X, Y, k=1000, t=1e-5, lam=0.01, alpha=0.1, regularization='L2')\n",
    "print(\"Learned beta with L2 regularization:\", beta_learned_L2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700cbd94-8ef5-4e5f-9540-7a76fade3569",
   "metadata": {},
   "source": [
    "# Exercise 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dc487c-2a08-4acc-82d8-4125a6c3017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Merge the linear regression code base created in Exercise 1 and the logistic regression code base created in\n",
    "# this Excercise and create an object oriented code base that maximises reuse of code across the algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "988a1efa-1e1a-4812-822d-5e95786e6d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression\n",
      "True Beta: [-0.58415645  1.07770459 -0.27891949 -1.1012186  -1.20653583 -0.81966296]\n",
      "Learned Beta: [-0.59214292  1.03485403 -0.12117309 -1.09020499 -1.23258499 -0.75494973]\n",
      "Final Cost: 0.37185026490464407\n",
      "\n",
      "Logistic Regression\n",
      "True Beta: [ 1.11219879  0.71721208 -0.10540705  0.96641242 -0.76874994  0.67392612]\n",
      "Learned Beta: [ 1.53654913  0.86848006  0.1464446   1.08059923 -0.73115567  0.36276731]\n",
      "Final Cost: 0.294610909780754\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Base class for Regression\n",
    "class BaseRegression:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000, tolerance=1e-6):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.tolerance = tolerance\n",
    "        self.beta = None\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pass\n",
    "    \n",
    "    def gradient_descent(self, X, Y):\n",
    "        n, m = X.shape\n",
    "        self.beta = np.random.randn(m)\n",
    "        prev_cost = float('inf')\n",
    "        \n",
    "        for iteration in range(self.epochs):\n",
    "            Y_pred = self.predict(X)\n",
    "            cost = self.compute_cost(Y, Y_pred)\n",
    "            \n",
    "            # Check for convergence\n",
    "            if np.abs(prev_cost - cost) < self.tolerance:\n",
    "                break\n",
    "            prev_cost = cost\n",
    "            \n",
    "            # Compute the gradient\n",
    "            gradient = (X.T @ (Y_pred - Y)) / n\n",
    "            self.beta -= self.learning_rate * gradient\n",
    "            \n",
    "        return self.beta, cost\n",
    "\n",
    "    def compute_cost(self, Y, Y_pred):\n",
    "        pass\n",
    "\n",
    "# Linear Regression Class inheriting BaseRegression\n",
    "class LinearRegression(BaseRegression):\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000, tolerance=1e-6):\n",
    "        super().__init__(learning_rate, epochs, tolerance)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X @ self.beta  # Linear prediction (dot product)\n",
    "\n",
    "    def compute_cost(self, Y, Y_pred):\n",
    "        # Mean squared error cost function for linear regression\n",
    "        n = len(Y)\n",
    "        cost = np.mean((Y_pred - Y) ** 2) / 2\n",
    "        return cost\n",
    "\n",
    "# Logistic Regression Class inheriting BaseRegression\n",
    "class LogisticRegression(BaseRegression):\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000, tolerance=1e-6):\n",
    "        super().__init__(learning_rate, epochs, tolerance)\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))  # Sigmoid activation function\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.sigmoid(X @ self.beta)  # Logistic prediction\n",
    "\n",
    "    def compute_cost(self, Y, Y_pred):\n",
    "        # Cross-entropy cost function for logistic regression\n",
    "        n = len(Y)\n",
    "        cost = -np.mean(Y * np.log(Y_pred) + (1 - Y) * np.log(1 - Y_pred))\n",
    "        return cost\n",
    "\n",
    "# Generate dataset function (shared)\n",
    "def generate_dataset_linear(sigma, n, m):\n",
    "    X = np.hstack((np.ones((n, 1)), np.random.randn(n, m)))  # Add column for intercept\n",
    "    beta = np.random.randn(m + 1)\n",
    "    noise = np.random.normal(0, sigma, n)\n",
    "    Y = X @ beta + noise\n",
    "    return X, Y, beta\n",
    "\n",
    "def generate_dataset_logistic(n, m):\n",
    "    X = np.hstack((np.ones((n, 1)), np.random.randn(n, m)))\n",
    "    beta = np.random.randn(m + 1)\n",
    "    linear_combination = X @ beta\n",
    "    prob = 1 / (1 + np.exp(-linear_combination))\n",
    "    Y = np.where(prob > 0.5, 1, 0)\n",
    "    return X, Y, beta\n",
    "\n",
    "#Linear Regression\n",
    "sigma = 1.0\n",
    "n = 100\n",
    "m = 5\n",
    "X_lin, Y_lin, beta_true_lin = generate_dataset_linear(sigma, n, m)\n",
    "linear_model = LinearRegression(learning_rate=0.01, epochs=1000, tolerance=1e-6)\n",
    "beta_learned_lin, final_cost_lin = linear_model.gradient_descent(X_lin, Y_lin)\n",
    "print(\"Linear Regression\")\n",
    "print(\"True Beta:\", beta_true_lin)\n",
    "print(\"Learned Beta:\", beta_learned_lin)\n",
    "print(\"Final Cost:\", final_cost_lin)\n",
    "\n",
    "#Logistic Regression\n",
    "X_log, Y_log, beta_true_log = generate_dataset_logistic(n, m)\n",
    "logistic_model = LogisticRegression(learning_rate=0.01, epochs=1000, tolerance=1e-6)\n",
    "beta_learned_log, final_cost_log = logistic_model.gradient_descent(X_log, Y_log)\n",
    "print(\"\\nLogistic Regression\")\n",
    "print(\"True Beta:\", beta_true_log)\n",
    "print(\"Learned Beta:\", beta_learned_log)\n",
    "print(\"Final Cost:\", final_cost_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71736d2-9970-40d1-83ef-536518b4c84e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
